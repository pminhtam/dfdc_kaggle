{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "# config.gpu_options.visible_device_list = \"0\" #only the gpu 0 is allowed\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "margin = 0.2\n",
    "from tqdm import tqdm\n",
    "from mtcnn import MTCNN\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tampm/miniconda3/envs/face/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract face from kaggle data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path0 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_0\"\n",
    "# path1 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_1\"\n",
    "# path2 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_2\"\n",
    "# path3 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_3\"\n",
    "# path48 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48\"\n",
    "# path49 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_49\"\n",
    "\n",
    "# path4 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_4\"\n",
    "# path5 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_5\"\n",
    "# path6 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_6\"\n",
    "# path7 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_7\"\n",
    "# path8 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_8\"\n",
    "# path9 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_9\"\n",
    "\n",
    "\n",
    "# path16 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_16\"\n",
    "# path17 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_17\"\n",
    "# path18 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_18\"\n",
    "# path19 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_19\"\n",
    "# path20 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_20\"\n",
    "\n",
    "path36 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_36\"\n",
    "path37 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_37\"\n",
    "path38 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_38\"\n",
    "path39 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_39\"\n",
    "path40 = \"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path36,path37,path38,path39,path40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(open(\"/hdd/tam/kaggle/train_sample_videos/metadata.json\"))\n",
    "# data0 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "# data1 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "# data2 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "# data3 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "# data48 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "# data49 = json.load(open(\"/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_48/metadata.json\"))\n",
    "IMGWIDTH = 128\n",
    "margin = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =0\n",
    "# real = 0\n",
    "# for vi in tqdm(data):\n",
    "#     if data[vi]['label'] == \"FAKE\":\n",
    "#         df+=1\n",
    "#     if data[vi]['label'] == 'REAL':\n",
    "#         real+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(join(\"/hdd/tam/kaggle/train_videos/real\", \"ncmpqwmnzb.mp4\"+\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2339/2339 [00:00<00:00, 35503.40it/s]\n",
      "  0%|          | 0/2655 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_36\n",
      "/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2655/2655 [6:43:28<00:00,  9.12s/it]  \n",
      "  0%|          | 0/2477 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/tam/kaggle/www.kaggle.com/c/16880/datadownload/dfdc_train_part_38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 171/2477 [31:17<6:27:20, 10.08s/it]"
     ]
    }
   ],
   "source": [
    "# save_interval = 6 # perform face detection every {save_interval} frames\n",
    "for path in paths:\n",
    "    data = json.load(open(join(path, \"metadata.json\")))\n",
    "    print(path)\n",
    "    for vi in tqdm(data): \n",
    "#         if data[vi]['label'] == \"REAL\":\n",
    "#             continue\n",
    "        if data[vi]['label'] == \"FAKE\":\n",
    "            if os.path.exists(join(\"/hdd/tam/kaggle/train_videos/df\", vi +\".pkl\")):\n",
    "                continue\n",
    "        if data[vi]['label'] == 'REAL':\n",
    "            if os.path.exists(join(\"/hdd/tam/kaggle/train_videos/real\", vi+\".pkl\")):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "        video = cv2.VideoCapture(join(path, vi))\n",
    "        success = True\n",
    "    #     success, vframe = video.read()\n",
    "        data_videos = []\n",
    "        save_interval = 13\n",
    "\n",
    "        success, image = video.read()\n",
    "        while success:\n",
    "    #         for i in range(0,video.__len__(),save_interval):\n",
    "            for i in range(save_interval):\n",
    "                success, image = video.read()\n",
    "                if not success:\n",
    "                    break\n",
    "    #         print(image.shape)\n",
    "    #         if image.all() ==None:\n",
    "    #             continue\n",
    "            try:\n",
    "                image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "            except:\n",
    "                continue\n",
    "        #         face_positions = face_recognition.face_locations(img)\n",
    "            face_positions = detector.detect_faces(image)\n",
    "            if len(face_positions) == 0:\n",
    "                continue\n",
    "            face_position =face_positions[0]['box']\n",
    "            x,y,w,h = face_position\n",
    "            offsetx = round(margin * (w))\n",
    "            offsety = round(margin * (h))\n",
    "            y0 = round(max(y - offsety, 0))\n",
    "            x1 = round(min(x + w + offsetx, image.shape[1]))\n",
    "            y1 = round(min(y+ h + offsety, image.shape[0]))\n",
    "            x0 = round(max(x - offsetx, 0))\n",
    "    #         print(x0,x1,y0,y1)\n",
    "            face = image[y0:y1,x0:x1]\n",
    "\n",
    "\n",
    "            face = cv2.resize(face,(IMGWIDTH,IMGWIDTH))\n",
    "    #         plt.imshow(face)\n",
    "    #         plt.show()\n",
    "            data_videos.append(face)\n",
    "            success, image = video.read()\n",
    "\n",
    "        data_videos = np.array(data_videos)\n",
    "        if data[vi]['label'] == \"FAKE\":\n",
    "            output = open(join(\"/hdd/tam/kaggle/train_videos/df\", vi +\".pkl\"),'wb')\n",
    "            pickle.dump(data_videos, output)\n",
    "            output.close()\n",
    "        if data[vi]['label'] == 'REAL':\n",
    "            output = open(join(\"/hdd/tam/kaggle/train_videos/real\", vi+\".pkl\"),'wb')\n",
    "            pickle.dump(data_videos, output)\n",
    "            output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract face from FaceForensics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(\"/hdd/tam/FaceForensics/data/original_sequences/youtube/c23/videos/*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [04:25<3:35:21, 13.16s/it]"
     ]
    }
   ],
   "source": [
    "for vi in tqdm(data):\n",
    "    vi = vi.split(\"/\")[-1]\n",
    "    video = cv2.VideoCapture(join(\"/hdd/tam/FaceForensics/data/original_sequences/youtube/c23/videos/\", vi))\n",
    "    success = True\n",
    "#     success, vframe = video.read()\n",
    "    data_videos = []\n",
    "    save_interval = 7\n",
    "    \n",
    "    success, image = video.read()\n",
    "    while success:\n",
    "        for i in range(save_interval):\n",
    "            success, image = video.read()\n",
    "            if not success:\n",
    "                break\n",
    "        try:\n",
    "            image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            continue\n",
    "    #         face_positions = face_recognition.face_locations(img)\n",
    "        face_positions = detector.detect_faces(image)\n",
    "        if len(face_positions) == 0:\n",
    "            continue\n",
    "        face_position =face_positions[0]['box']\n",
    "        x,y,w,h = face_position\n",
    "        offsetx = round(margin * (w))\n",
    "        offsety = round(margin * (h))\n",
    "        y0 = round(max(y - offsety, 0))\n",
    "        x1 = round(min(x + w + offsetx, image.shape[1]))\n",
    "        y1 = round(min(y+ h + offsety, image.shape[0]))\n",
    "        x0 = round(max(x - offsetx, 0))\n",
    "#         print(x0,x1,y0,y1)\n",
    "        face = image[y0:y1,x0:x1]\n",
    "\n",
    "\n",
    "        face = cv2.resize(face,(IMGWIDTH,IMGWIDTH))\n",
    "#         plt.imshow(face)\n",
    "#         plt.show()\n",
    "        data_videos.append(face)\n",
    "        success, image = video.read()\n",
    "\n",
    "    data_videos = np.array(data_videos)\n",
    "#     print(data_videos.shape)\n",
    "    output = open(join(\"/hdd/tam/kaggle/train_videos/real\", vi+\".pkl\"),'wb')\n",
    "    pickle.dump(data_videos, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f6970ecc78483f91d6d1da237a5343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\x0e\\x8f\\xc4mdat\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Video.from_file(\"/hdd/tam/FaceForensics/data/original_sequences/youtube/c23/videos/668.mp4\")\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
