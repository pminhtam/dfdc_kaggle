{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/tea1528/Non-Local-NN-Pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40f7997df5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_local\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLBlockND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, mode='embedded', \n",
    "                 dimension=3, bn_layer=True):\n",
    "        \"\"\"Implementation of Non-Local Block with 4 different pairwise functions but doesn't include subsampling trick\n",
    "        args:\n",
    "            in_channels: original channel size (1024 in the paper)\n",
    "            inter_channels: channel size inside the block if not specifed reduced to half (512 in the paper)\n",
    "            mode: supports Gaussian, Embedded Gaussian, Dot Product, and Concatenation\n",
    "            dimension: can be 1 (temporal), 2 (spatial), 3 (spatiotemporal)\n",
    "            bn_layer: whether to add batch norm\n",
    "        \"\"\"\n",
    "        super(NLBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [1, 2, 3]\n",
    "        \n",
    "        if mode not in ['gaussian', 'embedded', 'dot', 'concatenate']:\n",
    "            raise ValueError('`mode` must be one of `gaussian`, `embedded`, `dot` or `concatenate`')\n",
    "            \n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        # the channel size is reduced to half inside the block\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "        \n",
    "        # assign appropriate convolutional, max pool, and batch norm layers for different dimensions\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
    "            bn = nn.BatchNorm1d\n",
    "\n",
    "        # function g in the paper which goes through conv. with kernel size 1\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "\n",
    "        # add BatchNorm layer after the last conv layer\n",
    "        if bn_layer:\n",
    "            self.W_z = nn.Sequential(\n",
    "                    conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1),\n",
    "                    bn(self.in_channels)\n",
    "                )\n",
    "            # from section 4.1 of the paper, initializing params of BN ensures that the initial state of non-local block is identity mapping\n",
    "            nn.init.constant_(self.W_z[1].weight, 0)\n",
    "            nn.init.constant_(self.W_z[1].bias, 0)\n",
    "        else:\n",
    "            self.W_z = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1)\n",
    "\n",
    "            # from section 3.3 of the paper by initializing Wz to 0, this block can be inserted to any existing architecture\n",
    "            nn.init.constant_(self.W_z.weight, 0)\n",
    "            nn.init.constant_(self.W_z.bias, 0)\n",
    "\n",
    "        # define theta and phi for all operations except gaussian\n",
    "        if self.mode == \"embedded\" or self.mode == \"dot\" or self.mode == \"concatenate\":\n",
    "            self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "            self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "        \n",
    "        if self.mode == \"concatenate\":\n",
    "            self.W_f = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=self.inter_channels * 2, out_channels=1, kernel_size=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args\n",
    "            x: (N, C, T, H, W) for dimension=3; (N, C, H, W) for dimension 2; (N, C, T) for dimension 1\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # (N, C, THW)\n",
    "        # this reshaping and permutation is from the spacetime_nonlocal function in the original Caffe2 implementation\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        if self.mode == \"gaussian\":\n",
    "            theta_x = x.view(batch_size, self.in_channels, -1)\n",
    "            phi_x = x.view(batch_size, self.in_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "        elif self.mode == \"embedded\" or self.mode == \"dot\":\n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "        elif self.mode == \"concatenate\":\n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n",
    "            \n",
    "            h = theta_x.size(2)\n",
    "            w = phi_x.size(3)\n",
    "            theta_x = theta_x.repeat(1, 1, 1, w)\n",
    "            phi_x = phi_x.repeat(1, 1, h, 1)\n",
    "            \n",
    "            concat = torch.cat([theta_x, phi_x], dim=1)\n",
    "            f = self.W_f(concat)\n",
    "            f = f.view(f.size(0), f.size(2), f.size(3))\n",
    "        \n",
    "        if self.mode == \"gaussian\" or self.mode == \"embedded\":\n",
    "            f_div_C = F.softmax(f, dim=-1)\n",
    "        elif self.mode == \"dot\" or self.mode == \"concatenate\":\n",
    "            N = f.size(-1) # number of position in x\n",
    "            f_div_C = f / N\n",
    "        \n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        \n",
    "        # contiguous here just allocates contiguous chunk of memory\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        \n",
    "        W_y = self.W_z(y)\n",
    "        # residual connection\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet2D(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, non_local=False):\n",
    "        super(ResNet2D, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        \n",
    "        # add non-local block after layer 2\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2, non_local=non_local)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, non_local=False):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "\n",
    "        last_idx = len(strides)\n",
    "        if non_local:\n",
    "            last_idx = len(strides) - 1\n",
    "\n",
    "        for i in range(last_idx):\n",
    "            layers.append(block(self.in_planes, planes, strides[i]))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        if non_local:\n",
    "            layers.append(NLBlockND(in_channels=planes, dimension=2))\n",
    "            layers.append(block(self.in_planes, planes, strides[-1]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet2D56(non_local=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-56 model.\n",
    "    \"\"\"\n",
    "    return ResNet2D(BasicBlock, [9, 9, 9], non_local=non_local, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Variable(torch.randn(1, 3, 224, 224))\n",
    "net = resnet2D56()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             432\n",
      "       BatchNorm2d-2         [-1, 16, 224, 224]              32\n",
      "            Conv2d-3         [-1, 16, 224, 224]           2,304\n",
      "       BatchNorm2d-4         [-1, 16, 224, 224]              32\n",
      "            Conv2d-5         [-1, 16, 224, 224]           2,304\n",
      "       BatchNorm2d-6         [-1, 16, 224, 224]              32\n",
      "        BasicBlock-7         [-1, 16, 224, 224]               0\n",
      "            Conv2d-8         [-1, 16, 224, 224]           2,304\n",
      "       BatchNorm2d-9         [-1, 16, 224, 224]              32\n",
      "           Conv2d-10         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-11         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-12         [-1, 16, 224, 224]               0\n",
      "           Conv2d-13         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-14         [-1, 16, 224, 224]              32\n",
      "           Conv2d-15         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-16         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-17         [-1, 16, 224, 224]               0\n",
      "           Conv2d-18         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-19         [-1, 16, 224, 224]              32\n",
      "           Conv2d-20         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-21         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-22         [-1, 16, 224, 224]               0\n",
      "           Conv2d-23         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-24         [-1, 16, 224, 224]              32\n",
      "           Conv2d-25         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-26         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-27         [-1, 16, 224, 224]               0\n",
      "           Conv2d-28         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-29         [-1, 16, 224, 224]              32\n",
      "           Conv2d-30         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-31         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-32         [-1, 16, 224, 224]               0\n",
      "           Conv2d-33         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-34         [-1, 16, 224, 224]              32\n",
      "           Conv2d-35         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-36         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-37         [-1, 16, 224, 224]               0\n",
      "           Conv2d-38         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-39         [-1, 16, 224, 224]              32\n",
      "           Conv2d-40         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-41         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-42         [-1, 16, 224, 224]               0\n",
      "           Conv2d-43         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-44         [-1, 16, 224, 224]              32\n",
      "           Conv2d-45         [-1, 16, 224, 224]           2,304\n",
      "      BatchNorm2d-46         [-1, 16, 224, 224]              32\n",
      "       BasicBlock-47         [-1, 16, 224, 224]               0\n",
      "           Conv2d-48         [-1, 32, 112, 112]           4,608\n",
      "      BatchNorm2d-49         [-1, 32, 112, 112]              64\n",
      "           Conv2d-50         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-51         [-1, 32, 112, 112]              64\n",
      "      LambdaLayer-52         [-1, 32, 112, 112]               0\n",
      "       BasicBlock-53         [-1, 32, 112, 112]               0\n",
      "           Conv2d-54         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-55         [-1, 32, 112, 112]              64\n",
      "           Conv2d-56         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-57         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-58         [-1, 32, 112, 112]               0\n",
      "           Conv2d-59         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-60         [-1, 32, 112, 112]              64\n",
      "           Conv2d-61         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-62         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-63         [-1, 32, 112, 112]               0\n",
      "           Conv2d-64         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-65         [-1, 32, 112, 112]              64\n",
      "           Conv2d-66         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-67         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-68         [-1, 32, 112, 112]               0\n",
      "           Conv2d-69         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-70         [-1, 32, 112, 112]              64\n",
      "           Conv2d-71         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-72         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-73         [-1, 32, 112, 112]               0\n",
      "           Conv2d-74         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-75         [-1, 32, 112, 112]              64\n",
      "           Conv2d-76         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-77         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-78         [-1, 32, 112, 112]               0\n",
      "           Conv2d-79         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-80         [-1, 32, 112, 112]              64\n",
      "           Conv2d-81         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-82         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-83         [-1, 32, 112, 112]               0\n",
      "           Conv2d-84         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-85         [-1, 32, 112, 112]              64\n",
      "           Conv2d-86         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-87         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-88         [-1, 32, 112, 112]               0\n",
      "           Conv2d-89         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-90         [-1, 32, 112, 112]              64\n",
      "           Conv2d-91         [-1, 32, 112, 112]           9,216\n",
      "      BatchNorm2d-92         [-1, 32, 112, 112]              64\n",
      "       BasicBlock-93         [-1, 32, 112, 112]               0\n",
      "           Conv2d-94           [-1, 64, 56, 56]          18,432\n",
      "      BatchNorm2d-95           [-1, 64, 56, 56]             128\n",
      "           Conv2d-96           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-97           [-1, 64, 56, 56]             128\n",
      "      LambdaLayer-98           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-99           [-1, 64, 56, 56]               0\n",
      "          Conv2d-100           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-101           [-1, 64, 56, 56]             128\n",
      "          Conv2d-102           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-103           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-104           [-1, 64, 56, 56]               0\n",
      "          Conv2d-105           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-106           [-1, 64, 56, 56]             128\n",
      "          Conv2d-107           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-108           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-109           [-1, 64, 56, 56]               0\n",
      "          Conv2d-110           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-111           [-1, 64, 56, 56]             128\n",
      "          Conv2d-112           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-113           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-114           [-1, 64, 56, 56]               0\n",
      "          Conv2d-115           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-116           [-1, 64, 56, 56]             128\n",
      "          Conv2d-117           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-118           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-119           [-1, 64, 56, 56]               0\n",
      "          Conv2d-120           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-121           [-1, 64, 56, 56]             128\n",
      "          Conv2d-122           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-123           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-124           [-1, 64, 56, 56]               0\n",
      "          Conv2d-125           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-126           [-1, 64, 56, 56]             128\n",
      "          Conv2d-127           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-128           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-129           [-1, 64, 56, 56]               0\n",
      "          Conv2d-130           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-131           [-1, 64, 56, 56]             128\n",
      "          Conv2d-132           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-133           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-134           [-1, 64, 56, 56]               0\n",
      "          Conv2d-135           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-136           [-1, 64, 56, 56]             128\n",
      "          Conv2d-137           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-138           [-1, 64, 56, 56]             128\n",
      "      BasicBlock-139           [-1, 64, 56, 56]               0\n",
      "          Linear-140                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 853,018\n",
      "Trainable params: 853,018\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 499.19\n",
      "Params size (MB): 3.25\n",
      "Estimated Total Size (MB): 503.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net.cuda(),(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
